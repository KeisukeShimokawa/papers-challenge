{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SAM test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNGgZ1b5mhAM2N4YHKQWKYg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KeisukeShimokawa/papers-challenge/blob/master/src/cv/SAM/notebooks/SAM_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bqx2JCVEY80i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "fa9dcce8-1930-4111-9495-d77f0d0ee31f"
      },
      "source": [
        "!pip list | grep torch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch                         1.6.0+cu101    \n",
            "torchsummary                  1.5.1          \n",
            "torchtext                     0.3.1          \n",
            "torchvision                   0.7.0+cu101    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9Xxjw0aisRR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sR1_1OuZJUB",
        "colab_type": "text"
      },
      "source": [
        "まずは確認のために、以下の特徴マップをSAMの入力値と考える。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gny-uTDWiwou",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "756f2397-ffc8-4fe2-c79d-ba8f00fc6a93"
      },
      "source": [
        "inputs = torch.randn(10, 64, 16, 16)\n",
        "\n",
        "print(\"input feature map: \", inputs.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input feature map:  torch.Size([10, 64, 16, 16])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnQRrnKoaU2j",
        "colab_type": "text"
      },
      "source": [
        "以下ではパッチワイズのSelf-Attentionを考えていく。\n",
        "\n",
        "$$\\boldsymbol{y}_{i}=\\sum_{j \\in \\mathcal{R}(i)} \\alpha\\left(\\boldsymbol{x}_{R(i)}\\right)_{j} \\odot \\beta\\left(\\boldsymbol{x}_{j}\\right)$$\n",
        "\n",
        "このパッチワイズの計算のうち、左側のパッチ領域に対する計算では、アダマール積を計算する対象と形状を合わせる$\\gamma(\\cdot)$と関係性を計算する$\\delta(\\cdot)$に分解する。\n",
        "\n",
        "$$\\gamma\\left(\\delta\\left(\\boldsymbol{x}_{\\mathcal{R}(i)}\\right)\\right)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKPhOSlbi5bb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f31d8935-d432-451f-ef35-c7224b5285ce"
      },
      "source": [
        "in_planes  = inputs.shape[1] # 入力チャンネル\n",
        "rel_planes = in_planes // 16 # α\n",
        "out_planes = in_planes // 4  # β\n",
        "print(in_planes, rel_planes, out_planes)\n",
        "\n",
        "# 計算の設定を行う\n",
        "share_planes = 8\n",
        "kernel_size = 7\n",
        "stride = 1\n",
        "dilation = 1"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64 4 16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IT3gakV0cEOA",
        "colab_type": "text"
      },
      "source": [
        "以下の計算では、Attention計算に利用するパッチ領域に対して、計算コストを削減するため1x1畳み込み演算を使用してチャンネル数を小さくする。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAzxGymNjxj7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "5b92729f-e288-404c-d659-7d1c063eb7f6"
      },
      "source": [
        "# step1\n",
        "conv1 = nn.Conv2d(in_planes, rel_planes, kernel_size=1)\n",
        "conv2 = nn.Conv2d(in_planes, rel_planes, kernel_size=1)\n",
        "conv3 = nn.Conv2d(in_planes, out_planes, kernel_size=1)\n",
        "\n",
        "x1, x2, x3 = conv1(inputs), conv2(inputs), conv3(inputs)\n",
        "\n",
        "print(f\"x1: {x1.shape}\")\n",
        "print(f\"x2: {x2.shape}\")\n",
        "print(f\"x3: {x3.shape}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x1: torch.Size([10, 4, 16, 16])\n",
            "x2: torch.Size([10, 4, 16, 16])\n",
            "x3: torch.Size([10, 16, 16, 16])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-NgaCYYc4I_",
        "colab_type": "text"
      },
      "source": [
        "Self-Attention計算では、あるピクセル位置での特徴ベクトルと、その周辺のピクセルを含む特徴ベクトルに対してAttention計算を実施する。\n",
        "\n",
        "そこでまずはQueryとなる各ピクセル位置での特徴ベクトルを計算するために形状を変更する。\n",
        "\n",
        "具体的には、高さと幅の次元を1つにまとめる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BdhJCKckM_g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1344c2cf-e61a-4365-e111-7f3ce375f887"
      },
      "source": [
        "x1_reshape = x1.view(x1.shape[0], -1, 1, x1.shape[2] * x1.shape[3])\n",
        "\n",
        "print(f\"x1 reshape: {x1_reshape.shape}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x1 reshape: torch.Size([10, 4, 1, 256])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQIPzpymlzKe",
        "colab_type": "text"
      },
      "source": [
        "pytorchのUnfold関数の出力値\n",
        "\n",
        "$$\n",
        "(N, C \\times \\Pi(\\text { kernel_size }), L)\n",
        "$$\n",
        "\n",
        "この$L$とは以下の計算に従う。なお$d$は空間方向の次元数である。\n",
        "\n",
        "$$\n",
        "L=\\prod_{d}\\left[\\frac{\\text{spatial_size}[d]+2 \\times \\text{padding}[d]-\\text{dilation}[d] \\times\\left(\\text{kernel_size}[d]-1\\right)-1}{\\text{stride}[d]}+1\\right]\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfF1CyjGmaqM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4d19aaf6-223c-4f04-e6c6-848f440fda81"
      },
      "source": [
        "# paddingしたサイズと、Kernel_size-1のサイズが等しくなるので\n",
        "# 単純に入力された空間次元数をかけ合わせたものになる。(16x16=256)\n",
        "(((16+3*2) + 2 * 0 - (kernel_size - 1) - 1) + 1) ** 2"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACZX83i3mziY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9c56dd2e-e93e-4a60-c3cd-df2c12a4211a"
      },
      "source": [
        "# C x Product(kernel_size)\n",
        "4   * kernel_size ** 2"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "196"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mug0qZd5lsqv",
        "colab_type": "text"
      },
      "source": [
        "ここで行うPaddingとUnfoldのイメージは以下のようなものになる。\n",
        "\n",
        "![](https://www.dropbox.com/s/ieal9ylytzdga44/SAMPatch.jpg?dl=1)\n",
        "\n",
        "このイメージに従い、パッチ領域に合わせて特徴マップを変形させていく。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NO5x_a5JkkE9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "05a193dd-dc07-4df9-af22-c1c19749e6be"
      },
      "source": [
        "# 設定していたとおり、PaddingとUnfoldを初期化する\n",
        "unfold_x2 = nn.Unfold(kernel_size=kernel_size, dilation=dilation, padding=0, stride=stride)\n",
        "pad = nn.ReflectionPad2d(kernel_size // 2)\n",
        "\n",
        "# カーネルサイズに合わせてPaddingを設定する。\n",
        "# Paddingは通常の畳み込み演算を行う場合と考え方は変わらない\n",
        "x2_padded = pad(x2)\n",
        "print(f\"x2 padded: {x2_padded.shape}\")\n",
        "\n",
        "# これで上のイメージ図の右側のパッチ領域を計算できる\n",
        "x2_unfold = unfold_x2(x2_padded)\n",
        "print(f\"x2 unfold: {x2_unfold.shape}\")\n",
        "\n",
        "# ピクセルごとの特徴ベクトルを計算したテンソルと形状を合わせる\n",
        "x2_reshape = x2_unfold.view(x1_reshape.shape[0], -1, 1, x1_reshape.shape[-1])\n",
        "print(f\"s2 reshape: {x2_reshape.shape}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x2 padded: torch.Size([10, 4, 22, 22])\n",
            "x2 unfold: torch.Size([10, 196, 256])\n",
            "s2 reshape: torch.Size([10, 196, 1, 256])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcvDTxF3oV8c",
        "colab_type": "text"
      },
      "source": [
        "これで以下のように特徴ベクトルとパッチ領域のベクトルを取得できた。\n",
        "\n",
        "$$x_{1}:\\left[B, C, 1, H\\times{W}\\right]$$\n",
        "\n",
        "$$x_{2}:\\left[B, C\\times{\\text{kernel_size}^2}, 1, H\\times{W}\\right]$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WvAnPG_lA9c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "81d4b740-8050-488b-81cf-183e929a10b3"
      },
      "source": [
        "print(f\"x1 reshape: {x1_reshape.shape}\")\n",
        "print(f\"x2 reshape: {x2_reshape.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x1 reshape: torch.Size([10, 4, 1, 256])\n",
            "x2 reshape: torch.Size([10, 196, 1, 256])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRg2j1akpB_R",
        "colab_type": "text"
      },
      "source": [
        "次に、QueryとKeyの特徴マップを結合させる。\n",
        "\n",
        "$$\\text { Concatenation: } \\delta\\left(\\mathbf{x}_{\\mathcal{R}(i)}\\right)=\\left[\\varphi\\left(\\mathbf{x}_{i}\\right),\\left[\\psi\\left(\\mathbf{x}_{j}\\right)\\right]_{V j \\in \\mathcal{R}(i)}\\right]$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZXiiGJ8p3Cl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "37dcb823-1fa0-4382-c907-463b087d561f"
      },
      "source": [
        "w_concat = torch.cat((x1_reshape, x2_reshape), dim=1)\n",
        "print(f\"w concat: {w_concat.shape}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w concat: torch.Size([10, 200, 1, 256])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-UbVsTstywe",
        "colab_type": "text"
      },
      "source": [
        "次に結合させた特徴マップを、Valueとなる特徴マップと計算する前に、非線形な変換を行う。\n",
        "\n",
        "この際には論文中の以下の表に則り、2層のマッピング関数を使用する。\n",
        "\n",
        "![](https://www.dropbox.com/s/8zway0blqmeopc5/SAM%20mapping.png?dl=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4ZVjpy6vd44",
        "colab_type": "text"
      },
      "source": [
        "以下で計算した特徴マップを$\\beta$側に組み込むが、カーネルを共有する設定を設けているため、そのまま出力するのではなく、共有するカーネル分に合わせてチャンネル数を調整する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEM8O9CHpHbQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "364f1dc0-49d6-4adf-e4ca-fcffe8845e45"
      },
      "source": [
        "# 以下では1x1畳み込みを使用する。その際にはボトルネック構造を採用してチャンネル数を削減している。\n",
        "conv_w = nn.Sequential(\n",
        "    nn.BatchNorm2d(rel_planes * (1 + pow(kernel_size, 2))),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Conv2d(rel_planes * (1 + pow(kernel_size, 2)), out_planes // share_planes, kernel_size=1, bias=False),\n",
        "    nn.BatchNorm2d(out_planes // share_planes),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Conv2d(out_planes // share_planes, pow(kernel_size, 2) * out_planes // share_planes, kernel_size=1)\n",
        ")\n",
        "\n",
        "w_conv = conv_w(w_concat)\n",
        "print(f\"w conv: {w_conv.shape}\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w conv: torch.Size([10, 98, 1, 256])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3Nn-SFuxP1S",
        "colab_type": "text"
      },
      "source": [
        "次にカーネルサイズの大きさの次元を、他の次元に分離させ、各ピクセル位置でのパッチサイズに合わせた、チャンネル数に従う次元をもつ特徴マップを取得することができる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WglI_dZXvFnV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0e5d5a37-fd99-4872-97e4-23bd52cc2282"
      },
      "source": [
        "w_reshape = w_conv.view(x1_reshape.shape[0], -1, pow(kernel_size, 2), x1_reshape.shape[-1])\n",
        "print(f\"w reshape: {w_reshape.shape}\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w reshape: torch.Size([10, 2, 49, 256])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gvUvJYdx5hm",
        "colab_type": "text"
      },
      "source": [
        "なおカーネルサイズはストライドなど、高さと幅方向に同じ値を設定する場合は、以下のようにスカラー値からタプル形式のオブジェクトを取得できる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02lCmv33pQdW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "00854199-652d-4c55-97bc-0955aa51334f"
      },
      "source": [
        "from torch.nn.modules.utils import _pair\n",
        "\n",
        "\n",
        "pad_mode = 1\n",
        "\n",
        "\n",
        "kernel_size_pair = _pair(kernel_size)\n",
        "stride_pair = _pair(stride)\n",
        "padding_pair = _pair(kernel_size // 2)\n",
        "dilation_pair = _pair(dilation)\n",
        "\n",
        "print(kernel_size_pair)\n",
        "print(stride_pair)\n",
        "print(padding_pair)\n",
        "print(dilation_pair)\n",
        "print(pad_mode)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7, 7)\n",
            "(1, 1)\n",
            "(3, 3)\n",
            "(1, 1)\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLCbEuq2yLcY",
        "colab_type": "text"
      },
      "source": [
        "ここまでで$\\alpha$部分と$\\beta$部分を計算することができた。\n",
        "\n",
        "ここで`shared_kernel`に従ってAttentionの重みを共有する。\n",
        "\n",
        "$$\\beta\\left(\\boldsymbol{x}_{j}\\right):\\left[B, C, H, W\\right]$$\n",
        "\n",
        "$$\\alpha\\left(\\boldsymbol{x}_{R(i)}\\right)_{j}:\\left[B, \\frac{C}{\\text{shared_kernel}}, \\text{kernel_size}^2, H\\times{W}\\right]$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cah020ryyLXR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kq4srIRQtJsj",
        "colab_type": "text"
      },
      "source": [
        "## Autual Sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTLXqrTGyhug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class SAM(nn.Module):\n",
        "    def __init__(self, sa_type, in_planes, rel_planes, out_planes, share_planes, kernel_size=3, stride=1, dilation=1):\n",
        "        super(SAM, self).__init__()\n",
        "        self.sa_type, self.kernel_size, self.stride = sa_type, kernel_size, stride\n",
        "        self.conv1 = nn.Conv2d(in_planes, rel_planes, kernel_size=1)\n",
        "        self.conv2 = nn.Conv2d(in_planes, rel_planes, kernel_size=1)\n",
        "        self.conv3 = nn.Conv2d(in_planes, out_planes, kernel_size=1)\n",
        "        self.conv_w = nn.Sequential(nn.BatchNorm2d(rel_planes * (pow(kernel_size, 2) + 1)),\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Conv2d(rel_planes * (pow(kernel_size, 2) + 1), out_planes // share_planes, kernel_size=1, bias=False),\n",
        "                                    nn.BatchNorm2d(out_planes // share_planes),\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Conv2d(out_planes // share_planes, pow(kernel_size, 2) * out_planes // share_planes, kernel_size=1))\n",
        "        self.unfold_i = nn.Unfold(kernel_size=1, dilation=dilation, padding=0, stride=stride)\n",
        "        self.unfold_j = nn.Unfold(kernel_size=kernel_size, dilation=dilation, padding=0, stride=stride)\n",
        "        self.pad = nn.ReflectionPad2d(kernel_size // 2)\n",
        "        # self.aggregation = Aggregation(kernel_size, stride, (dilation * (kernel_size - 1) + 1) // 2, dilation, pad_mode=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1, x2, x3 = self.conv1(x), self.conv2(x), self.conv3(x)\n",
        "        if self.sa_type == 0:  # pairwise\n",
        "            p = self.conv_p(position(x.shape[2], x.shape[3], x.is_cuda))\n",
        "            w = self.softmax(self.conv_w(torch.cat([self.subtraction2(x1, x2), self.subtraction(p).repeat(x.shape[0], 1, 1, 1)], 1)))\n",
        "        else:  # patchwise\n",
        "            if self.stride != 1:\n",
        "                x1 = self.unfold_i(x1)\n",
        "            x1 = x1.view(x.shape[0], -1, 1, x.shape[2]*x.shape[3])\n",
        "            x2 = self.unfold_j(self.pad(x2)).view(x.shape[0], -1, 1, x1.shape[-1])\n",
        "            w = self.conv_w(torch.cat([x1, x2], 1)).view(x.shape[0], -1, pow(self.kernel_size, 2), x1.shape[-1])\n",
        "        print(x3.shape)\n",
        "        print(w.shape)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUnJgvLRyscH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "8fc435b2-b586-4a1d-efd2-e7119326b50a"
      },
      "source": [
        "sam_sample = SAM(sa_type=1,\n",
        "                 in_planes=in_planes,\n",
        "                 rel_planes=rel_planes,\n",
        "                 out_planes=out_planes,\n",
        "                 share_planes=share_planes,\n",
        "                 kernel_size=kernel_size,\n",
        "                 stride=stride,\n",
        "                 dilation=dilation)\n",
        "\n",
        "sam_sample"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SAM(\n",
              "  (conv1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n",
              "  (conv2): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n",
              "  (conv3): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
              "  (conv_w): Sequential(\n",
              "    (0): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(200, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    (3): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Conv2d(2, 98, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (unfold_i): Unfold(kernel_size=1, dilation=1, padding=0, stride=1)\n",
              "  (unfold_j): Unfold(kernel_size=7, dilation=1, padding=0, stride=1)\n",
              "  (pad): ReflectionPad2d((3, 3, 3, 3))\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xT0Xtooky9Nx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9e715ae8-f5cb-4d98-af40-29652eed6872"
      },
      "source": [
        "_ = sam_sample(inputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 16, 16, 16])\n",
            "torch.Size([10, 2, 49, 256])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6ScwVugzJz0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}