{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KeisukeShimokawa/papers-challenge/blob/master/src/gan/FQGAN/notebooks/VectorQuantizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append(os.path.join(os.getcwd(), \"../\"))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tw20n75RRmW_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqH-2AXDRmSw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torchvision.utils import make_grid"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MS9DacfKAIBX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from config import Config\n",
        "from datasets import data_utils\n",
        "from training.logger import Logger\n",
        "from training.utils import set_seed, count_parameters_float32\n",
        "from training.metric_log import MetricLog\n",
        "from models.FQGAN_64 import Generator, Discriminator\n",
        "from models.losses import ortho_reg, ProbLoss"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGr6_novRYFX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VectorQuantizer(nn.Module):\n",
        "\n",
        "    def __init__(self, emb_dim=64, num_emb=2**10, commitment=0.25):\n",
        "        super(VectorQuantizer, self).__init__()\n",
        "\n",
        "        self.emb_dim = emb_dim\n",
        "        self.num_emb = num_emb\n",
        "        self.commitment = commitment\n",
        "        self.embedding = nn.Parameter(torch.randn(emb_dim, num_emb))\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        # [B, C=D, H, W] --> [B, H, W, C=D]\n",
        "        inputs = inputs.permute(0, 2, 3, 1).contiguous()\n",
        "        inputs_shape = inputs.size()\n",
        "\n",
        "        # [B, H, W, D] --> [N(=BxHxW), D]\n",
        "        flatten = inputs.view(-1, self.emb_dim)\n",
        "\n",
        "        # distance d(H[N, D], E[D, K]) --> d[N, K]\n",
        "        # each element show the distance between Hj and Ei\n",
        "        distance = (\n",
        "            flatten.pow(2).sum(1, keepdim=True)\n",
        "            -2 * flatten @ self.embedding\n",
        "            + self.embedding.pow(2).sum(0, keepdim=True)\n",
        "        )\n",
        "\n",
        "        # embedding_idx: [N, K] --> [N, ]\n",
        "        embedding_idx = torch.argmin(distance, dim=1)\n",
        "        # embedding_idx: [N, ] --> [B, H, W, ]\n",
        "        embedding_idx = embedding_idx.view(*inputs_shape[:-1])\n",
        "        # quantize: [B, H, W, ] --> [B, H, W, D]\n",
        "        quantize = F.embedding(embedding_idx, self.embedding.transpose(0, 1))\n",
        "\n",
        "        # loss\n",
        "        e_latent_loss = F.mse_loss(quantize.detach(), inputs)\n",
        "        q_latent_loss = F.mse_loss(quantize, inputs.detach())\n",
        "        loss = q_latent_loss + self.commitment * e_latent_loss\n",
        "\n",
        "        quantize = inputs + (quantize - inputs).detach()\n",
        "        quantize = quantize.permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        return quantize, loss, embedding_idx"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmXOLkidjBwI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VectorQuantizerEMA(nn.Module):\n",
        "\n",
        "    def __init__(self, emb_dim=64, num_emb=2**10, commitment=1.0, decay=0.9, eps=1e-5):\n",
        "        super(VectorQuantizerEMA, self).__init__()\n",
        "\n",
        "        self.emb_dim = emb_dim\n",
        "        self.num_emb = num_emb\n",
        "        self.commitment = commitment\n",
        "        self.decay = decay\n",
        "        self.eps = eps\n",
        "\n",
        "        embedding = nn.Parameter(torch.randn(emb_dim, num_emb))\n",
        "        self.register_buffer(\"embedding\", embedding)\n",
        "        self.register_buffer(\"cluster_size\", torch.zeros(self.num_emb))\n",
        "        self.register_buffer(\"ema_embedding\", self.embedding.clone())\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # [B, C=D, H, W] --> [B, H, W, C=D]\n",
        "        inputs = inputs.permute(0, 2, 3, 1).contiguous()\n",
        "        inputs_shape = inputs.size()\n",
        "\n",
        "        # [B, H, W, D] --> [N(=BxHxW), D]\n",
        "        flatten = inputs.view(-1, self.emb_dim)\n",
        "\n",
        "        # distance d(H[N, D], E[D, K]) --> d[N, K]\n",
        "        # each element show the distance between Hj and Ei\n",
        "        distance = (\n",
        "            flatten.pow(2).sum(1, keepdim=True)\n",
        "            -2 * flatten @ self.embedding\n",
        "            + self.embedding.pow(2).sum(0, keepdim=True)\n",
        "        )\n",
        "\n",
        "        # embedding_idx: [N, K] --> [N, ]\n",
        "        embedding_idx = torch.argmin(distance, dim=1)\n",
        "        # embedding_onthot: [N, ] --> [N, K]\n",
        "        embedding_onehot = F.one_hot(embedding_idx, self.num_emb).type(flatten.dtype)\n",
        "        # embedding_idx: [N, ] --> [B, H, W, ]\n",
        "        embedding_idx = embedding_idx.view(*inputs_shape[:-1])\n",
        "        # quantize: [B, H, W, ] --> [B, H, W, D]\n",
        "        quantize = F.embedding(embedding_idx, self.embedding.transpose(0, 1))\n",
        "\n",
        "        if self.training:\n",
        "            self.cluster_size.mul_(self.decay).add_(\n",
        "                1-self.decay, embedding_onehot.sum(0)\n",
        "            )\n",
        "            dw = flatten.transpose(0, 1) @ embedding_onehot\n",
        "            self.ema_embedding.data.mul_(self.decay).add_(1-self.decay, dw)\n",
        "            n = self.cluster_size.sum()\n",
        "            smoother_cluster_size = (\n",
        "                (self.cluster_size + self.eps) / (n + self.num_emb * self.eps) * n\n",
        "            )\n",
        "            embedding_norm = self.ema_embedding / smoother_cluster_size.unsqueeze(0)\n",
        "            self.embedding.data.copy_(embedding_norm)\n",
        "\n",
        "        # loss\n",
        "        e_latent_loss = F.mse_loss(quantize.detach(), inputs)\n",
        "        loss = self.commitment * e_latent_loss\n",
        "\n",
        "        quantize = inputs + (quantize - inputs).detach()\n",
        "        quantize = quantize.permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        return quantize, loss, embedding_idx"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLnoXz5Rw30S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# custom weights initialization called on netG and netD\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaAvRjCswGbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, nz, ngf, nc):\n",
        "        super(Generator, self).__init__()\n",
        "        self.nz = nz\n",
        "        self.main = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "            nn.ConvTranspose2d(     nz, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*8) x 4 x 4\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*4) x 8 x 8\n",
        "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*2) x 16 x 16\n",
        "            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf) x 32 x 32\n",
        "            nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "            # state size. (nc) x 64 x 64\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        input = input.view(-1, self.nz, 1, 1)\n",
        "        output = self.main(input)\n",
        "        return output\n"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVlBn07RwGYC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "79e2d250-9fd7-4cda-880e-fd9b320f48bf"
      },
      "source": [
        "Generator(100, 64, 3)(torch.randn(10, 100)).shape"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "torch.Size([10, 3, 64, 64])"
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BP5cvbAOAKOs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "294498d8-c483-4a9b-def5-b988c51cce03"
      },
      "source": [
        "count_parameters_float32(Generator(100, 64, 3))"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "13.64404296875"
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQIPs_GLw0aX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, nc, ndf, emb_dim, num_emb):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            # input is (nc) x 64 x 64\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True))\n",
        "        \n",
        "        # self.conv_pre1 = nn.Conv2d(ndf * 1, emb_dim, 1, 1, 0)\n",
        "        # self.vq1 = VectorQuantizerEMA(emb_dim, num_emb)\n",
        "        # self.conv_pos1 = nn.Conv2d(emb_dim, ndf * 1, 1, 1, 0)\n",
        "\n",
        "        self.layer2 = nn.Sequential(\n",
        "            # state size. (ndf) x 32 x 32\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True))\n",
        "        \n",
        "        # self.conv_pre2 = nn.Conv2d(ndf * 2, emb_dim, 1, 1, 0)\n",
        "        # self.vq2 = VectorQuantizerEMA(emb_dim, num_emb)\n",
        "        # self.conv_pos2 = nn.Conv2d(emb_dim, ndf * 2, 1, 1, 0)\n",
        "\n",
        "        self.layer3 = nn.Sequential(\n",
        "            # state size. (ndf*2) x 16 x 16\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True))\n",
        "\n",
        "        self.conv_pre3 = nn.Conv2d(ndf * 4, emb_dim, 1, 1, 0)\n",
        "        self.vq = VectorQuantizerEMA(emb_dim, num_emb)\n",
        "        self.conv_pos3 = nn.Conv2d(emb_dim, ndf * 4, 1, 1, 0)\n",
        "\n",
        "        self.layer4 = nn.Sequential(\n",
        "            # state size. (ndf*4) x 8 x 8\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True))\n",
        "\n",
        "        # self.conv_pre4 = nn.Conv2d(ndf * 8, emb_dim, 1, 1, 0)\n",
        "        # self.vq4 = VectorQuantizerEMA(emb_dim, num_emb)\n",
        "        # self.conv_pos4 = nn.Conv2d(emb_dim, ndf * 8, 1, 1, 0)\n",
        "\n",
        "        self.layer5 = nn.Sequential(\n",
        "            # state size. (ndf*8) x 4 x 4\n",
        "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        total_loss = torch.tensor(0.0)\n",
        "\n",
        "        output = self.layer1(input)\n",
        "        # pre = self.conv_pre1(output)\n",
        "        # quantize, loss, embedding_idx = self.vq(pre); total_loss += loss\n",
        "        # output = self.conv_pos1(quantize)\n",
        "\n",
        "        output = self.layer2(output)\n",
        "        # pre = self.conv_pre2(output)\n",
        "        # quantize, loss, embedding_idx = self.vq(pre); total_loss += loss\n",
        "        # output = self.conv_pos2(quantize)\n",
        "\n",
        "        output = self.layer3(output)\n",
        "        pre = self.conv_pre3(output)\n",
        "        quantize, loss, embedding_idx = self.vq(pre); total_loss += loss\n",
        "        output = self.conv_pos3(quantize)\n",
        "\n",
        "        output = self.layer4(output)\n",
        "        # pre = self.conv_pre4(output)\n",
        "        # quantize, loss, embedding_idx = self.vq(pre); total_loss += loss\n",
        "        # output = self.conv_pos4(quantize)\n",
        "\n",
        "        output = self.layer5(output)\n",
        "        return output.view(-1, 1).squeeze(1), pre, quantize, loss, embedding_idx"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGsa2Mi0w0Wy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b423cd4d-cadd-4d40-82cf-d5f8289a0998"
      },
      "source": [
        "Discriminator(3, 64, 128, 2**10)(torch.randn(10, 3, 64, 64))[0].shape, \\\n",
        "Discriminator(3, 64, 128, 2**10)(torch.randn(10, 3, 64, 64))[1].shape, \\\n",
        "Discriminator(3, 64, 128, 2**10)(torch.randn(10, 3, 64, 64))[2].shape, \\\n",
        "# Discriminator(3, 64, 128, 2**10)(torch.randn(10, 3, 64, 64))[3].shape, \\\n",
        "Discriminator(3, 64, 128, 2**10)(torch.randn(10, 3, 64, 64))[4].shape"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "torch.Size([10, 8, 8])"
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OD0BWRyO3Rq2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9ee0f449-f0e2-494a-9aff-94a5e39f3220"
      },
      "source": [
        "Discriminator(3, 64, 128, 2**10)(torch.randn(10, 3, 64, 64))[3]"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "tensor(0.7616, grad_fn=<MulBackward0>)"
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wfxu_N98AOpL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0c80b290-aff4-40f3-eb47-c605d860b1b5"
      },
      "source": [
        "count_parameters_float32(Discriminator(3, 64, 128, 2**10))"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "10.80126953125"
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOefEwxOw0Ti",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "287501e0-d1c3-412f-9e19-260bc3861ef2"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "device(type='cuda', index=0)"
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mWwry5ty9qU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "79753c3f-2b92-436b-fe8a-65d3f82a597b"
      },
      "source": [
        "nz = 100\n",
        "ngf = 64\n",
        "ndf = 64\n",
        "nc = 3\n",
        "ortho = 1e-4\n",
        "emb_dim = 256\n",
        "num_emb = 2**10\n",
        "dataset_name = \"celeba\"\n",
        "n_epochs = 100\n",
        "n_dis = 4\n",
        "batch_size = 128\n",
        "num_workers = 4\n",
        "\n",
        "netG = Generator(nz, ngf, nc).to(device)\n",
        "netG.apply(weights_init)\n",
        "\n",
        "netD = Discriminator(nc, ndf, emb_dim, num_emb).to(device)\n",
        "netD.apply(weights_init)"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "Discriminator(\n  (layer1): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n  )\n  (layer2): Sequential(\n    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n  )\n  (layer3): Sequential(\n    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n  )\n  (conv_pre3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n  (vq): VectorQuantizerEMA()\n  (conv_pos3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n  (layer4): Sequential(\n    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n  )\n  (layer5): Sequential(\n    (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n  )\n)"
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MyEfxody9ni",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "6727ecf8889443a7bbff63a62667925e",
            "0da1406e59af48058f047100c7d1a886",
            "e292a8af7ec44cd8ae08227170a7cd4f",
            "fe61109823124397b661ce95f3475f20",
            "e3c0c1467146463ead540a6e72ee3eee",
            "27e54298f74f412893fe519cbba84f25",
            "d887e3f6974c4195b85e129270d23087",
            "ef339de6ad474e7f8cce5ba593608617"
          ]
        },
        "outputId": "3c4c967f-4ca4-4537-afd2-74da7a41186a"
      },
      "source": [
        "from torchvision import datasets as dsets\n",
        "from torchvision import transforms\n",
        "from torchvision import utils as vutils"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [],
      "source": [
        "if dataset_name == \"cifar10\":\n",
        "    dataset = dsets.CIFAR10(\n",
        "        root=\"./data\", \n",
        "        download=True,\n",
        "        transform=transforms.Compose([\n",
        "            transforms.Resize((64, 64)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ])\n",
        "    )\n",
        "\n",
        "elif dataset_name == \"cifar100\":\n",
        "    dataset = dsets.CIFAR100(\n",
        "        root=\"./data\", \n",
        "        download=True,\n",
        "        transform=transforms.Compose([\n",
        "            transforms.Resize((64, 64)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ])\n",
        "    )\n",
        "\n",
        "elif dataset_name == \"celeba\":\n",
        "    dataset = dsets.ImageFolder(\n",
        "        root=\"../../data/celeba\", \n",
        "        transform=transforms.Compose([\n",
        "            transforms.Resize((64, 64)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ])\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset, \n",
        "    batch_size=batch_size,\n",
        "    shuffle=True, \n",
        "    num_workers=int(num_workers))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "torch.Size([128, 3, 64, 64])\ntorch.Size([128])\n"
        }
      ],
      "source": [
        "inputs, labels = next(iter(dataloader))\n",
        "\n",
        "print(inputs.shape)\n",
        "print(labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnyHKHXty9ku",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "fixed_noise = torch.randn(128, nz, 1, 1, device=device)\n",
        "real_label = 1\n",
        "fake_label = 0"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emMLlGOVy9iQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizerD = optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYEm8ybC4V6K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "output_dir = Path(\"./output\").joinpath(time.strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
        "output_dir.mkdir(parents=True, exist_ok=True)"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Df_A5DHiy9fJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7121dfa8-c04f-4f1c-bc67-6de82db01e4a",
        "tags": [
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend"
        ]
      },
      "source": [
        "for epoch in range(n_epochs):\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "\n",
        "        # for i in range(n_dis):\n",
        "        ############################\n",
        "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "        ###########################\n",
        "        # train with real\n",
        "        netD.zero_grad()\n",
        "        real_cpu = data[0].to(device)\n",
        "        batch_size = real_cpu.size(0)\n",
        "        label = torch.full((batch_size,), real_label, device=device)\n",
        "        # Quantize\n",
        "        output, pre_real, quant_real, loss_quant_real, embedding_idx_real = netD(real_cpu)\n",
        "        errD_real = criterion(output, label)\n",
        "        lossD_real = errD_real +  loss_quant_real\n",
        "        lossD_real.backward()\n",
        "        D_x = output.sigmoid().mean().item()\n",
        "\n",
        "        # train with fake\n",
        "        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "        fake = netG(noise)\n",
        "        label.fill_(fake_label)\n",
        "        # Quantize\n",
        "        output, pre_fake, quant_fake, loss_quant_fake, embedding_idx_fake = netD(fake.detach())\n",
        "        errD_fake = criterion(output, label)\n",
        "        lossD_fake = errD_fake +  loss_quant_fake\n",
        "        lossD_fake.backward()\n",
        "        D_G_z1 = output.sigmoid().mean().item()\n",
        "\n",
        "        # total loss\n",
        "        errD = lossD_real + lossD_fake\n",
        "        if ortho:\n",
        "            ortho_reg(netD)\n",
        "\n",
        "        optimizerD.step()\n",
        "\n",
        "        ############################\n",
        "        # (2) Update G network: maximize log(D(G(z)))\n",
        "        ###########################\n",
        "        if i % n_dis == 0:\n",
        "            netG.zero_grad()\n",
        "            label.fill_(real_label)  # fake labels are real for generator cost\n",
        "            # Quantize\n",
        "            output, pre_G, quant_G, loss_quant_G, embedding_idx_G = netD(fake)\n",
        "            errG = criterion(output, label)\n",
        "            lossG = errG + loss_quant_G\n",
        "            lossG.backward()\n",
        "            D_G_z2 = output.sigmoid().mean().item()\n",
        "\n",
        "            if ortho:\n",
        "                ortho_reg(netG)\n",
        "            optimizerG.step()\n",
        "\n",
        "        \n",
        "        \n",
        "        if i % (100*n_dis) == 0:\n",
        "            print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
        "                    % (epoch, n_epochs, i, len(dataloader),\n",
        "                        errD.item(), lossG.item(), D_x, D_G_z1, D_G_z2))\n",
        "\n",
        "            vutils.save_image(real_cpu,\n",
        "                    '%s/real_samples.png' % output_dir,\n",
        "                    normalize=True)\n",
        "            fake = netG(fixed_noise)\n",
        "            vutils.save_image(fake.detach(),\n",
        "                    '%s/amp_fake_samples_epoch_%03d.png' % (output_dir, epoch),\n",
        "                    normalize=True)\n",
        "\n",
        "    # do checkpointing\n",
        "    torch.save(netG.state_dict(), '%s/netG_epoch_%d.pth' % (output_dir, epoch))\n",
        "    torch.save(netD.state_dict(), '%s/netD_epoch_%d.pth' % (output_dir, epoch))"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "0261 D(x): 0.9960 D(G(z)): 0.3677 / 0.0056\n[42/100][800/1583] Loss_D: 0.1342 Loss_G: 3.7202 D(x): 0.9971 D(G(z)): 0.0448 / 0.0413\n[42/100][1200/1583] Loss_D: 0.1674 Loss_G: 4.4171 D(x): 0.9938 D(G(z)): 0.0661 / 0.0219\n[43/100][0/1583] Loss_D: 0.1426 Loss_G: 4.4738 D(x): 0.9669 D(G(z)): 0.0229 / 0.0203\n[43/100][400/1583] Loss_D: 0.1210 Loss_G: 4.6955 D(x): 0.9836 D(G(z)): 0.0155 / 0.0246\n[43/100][800/1583] Loss_D: 0.2977 Loss_G: 4.6139 D(x): 0.8309 D(G(z)): 0.0069 / 0.0209\n[43/100][1200/1583] Loss_D: 1.7202 Loss_G: 2.1675 D(x): 0.7961 D(G(z)): 0.6513 / 0.2818\n[44/100][0/1583] Loss_D: 0.2190 Loss_G: 3.6193 D(x): 0.9582 D(G(z)): 0.0749 / 0.0489\n[44/100][400/1583] Loss_D: 0.2252 Loss_G: 5.3293 D(x): 0.8909 D(G(z)): 0.0093 / 0.0116\n[44/100][800/1583] Loss_D: 0.3539 Loss_G: 2.4852 D(x): 0.8010 D(G(z)): 0.0067 / 0.1353\n[44/100][1200/1583] Loss_D: 0.1646 Loss_G: 5.3116 D(x): 0.9365 D(G(z)): 0.0128 / 0.0087\n[45/100][0/1583] Loss_D: 0.1460 Loss_G: 5.3888 D(x): 0.9556 D(G(z)): 0.0145 / 0.0094\n[45/100][400/1583] Loss_D: 0.1162 Loss_G: 4.4782 D(x): 0.9980 D(G(z)): 0.0299 / 0.0203\n[45/100][800/1583] Loss_D: 0.2869 Loss_G: 3.6888 D(x): 0.9786 D(G(z)): 0.1494 / 0.0470\n[45/100][1200/1583] Loss_D: 0.3234 Loss_G: 3.2357 D(x): 0.9874 D(G(z)): 0.1818 / 0.0686\n[46/100][0/1583] Loss_D: 0.2149 Loss_G: 3.7964 D(x): 0.8948 D(G(z)): 0.0129 / 0.0482\n[46/100][400/1583] Loss_D: 0.1221 Loss_G: 5.0114 D(x): 0.9890 D(G(z)): 0.0279 / 0.0138\n[46/100][800/1583] Loss_D: 0.1668 Loss_G: 5.4083 D(x): 0.9285 D(G(z)): 0.0075 / 0.0080\n[46/100][1200/1583] Loss_D: 0.1190 Loss_G: 5.0544 D(x): 0.9784 D(G(z)): 0.0160 / 0.0124\n[47/100][0/1583] Loss_D: 0.3387 Loss_G: 3.4025 D(x): 0.8851 D(G(z)): 0.1070 / 0.0543\n[47/100][400/1583] Loss_D: 0.1491 Loss_G: 4.9122 D(x): 0.9729 D(G(z)): 0.0392 / 0.0147\n[47/100][800/1583] Loss_D: 0.2529 Loss_G: 3.6161 D(x): 0.9960 D(G(z)): 0.1415 / 0.0444\n[47/100][1200/1583] Loss_D: 0.1227 Loss_G: 6.2405 D(x): 0.9626 D(G(z)): 0.0051 / 0.0046\n[48/100][0/1583] Loss_D: 0.1655 Loss_G: 3.4921 D(x): 0.9959 D(G(z)): 0.0682 / 0.0547\n[48/100][400/1583] Loss_D: 0.1502 Loss_G: 8.7291 D(x): 0.9413 D(G(z)): 0.0005 / 0.0005\n[48/100][800/1583] Loss_D: 0.1032 Loss_G: 5.5896 D(x): 0.9916 D(G(z)): 0.0133 / 0.0074\n[48/100][1200/1583] Loss_D: 0.1172 Loss_G: 5.4098 D(x): 0.9768 D(G(z)): 0.0161 / 0.0090\n[49/100][0/1583] Loss_D: 0.2861 Loss_G: 4.6346 D(x): 0.8824 D(G(z)): 0.0551 / 0.0223\n[49/100][400/1583] Loss_D: 0.1064 Loss_G: 4.7033 D(x): 0.9976 D(G(z)): 0.0229 / 0.0171\n[49/100][800/1583] Loss_D: 0.0945 Loss_G: 6.0271 D(x): 0.9930 D(G(z)): 0.0101 / 0.0050\n[49/100][1200/1583] Loss_D: 0.1192 Loss_G: 5.1968 D(x): 0.9912 D(G(z)): 0.0249 / 0.0107\n[50/100][0/1583] Loss_D: 0.1764 Loss_G: 4.6310 D(x): 0.9869 D(G(z)): 0.0686 / 0.0195\n[50/100][400/1583] Loss_D: 0.1214 Loss_G: 5.1361 D(x): 0.9792 D(G(z)): 0.0200 / 0.0124\n[50/100][800/1583] Loss_D: 0.0947 Loss_G: 5.2880 D(x): 0.9959 D(G(z)): 0.0147 / 0.0085\n[50/100][1200/1583] Loss_D: 0.1151 Loss_G: 6.6223 D(x): 0.9754 D(G(z)): 0.0062 / 0.0038\n[51/100][0/1583] Loss_D: 0.1163 Loss_G: 7.4467 D(x): 0.9686 D(G(z)): 0.0029 / 0.0015\n[51/100][400/1583] Loss_D: 0.1067 Loss_G: 4.3544 D(x): 0.9953 D(G(z)): 0.0229 / 0.0239\n[51/100][800/1583] Loss_D: 0.1806 Loss_G: 5.7334 D(x): 0.9147 D(G(z)): 0.0069 / 0.0073\n[51/100][1200/1583] Loss_D: 0.1499 Loss_G: 4.0152 D(x): 0.9822 D(G(z)): 0.0413 / 0.0404\n[52/100][0/1583] Loss_D: 0.2820 Loss_G: 5.8689 D(x): 0.9944 D(G(z)): 0.1606 / 0.0053\n[52/100][400/1583] Loss_D: 0.1000 Loss_G: 6.8274 D(x): 0.9842 D(G(z)): 0.0043 / 0.0025\n[52/100][800/1583] Loss_D: 2.1921 Loss_G: 10.7778 D(x): 0.9999 D(G(z)): 0.7887 / 0.0001\n[52/100][1200/1583] Loss_D: 0.1690 Loss_G: 4.2068 D(x): 0.9678 D(G(z)): 0.0466 / 0.0303\n[53/100][0/1583] Loss_D: 0.1242 Loss_G: 5.0952 D(x): 0.9737 D(G(z)): 0.0172 / 0.0134\n[53/100][400/1583] Loss_D: 0.0906 Loss_G: 5.5250 D(x): 0.9975 D(G(z)): 0.0092 / 0.0080\n[53/100][800/1583] Loss_D: 0.2470 Loss_G: 6.6385 D(x): 0.8994 D(G(z)): 0.0034 / 0.0033\n[53/100][1200/1583] Loss_D: 0.3267 Loss_G: 3.6084 D(x): 0.9764 D(G(z)): 0.1768 / 0.0486\n[54/100][0/1583] Loss_D: 0.0989 Loss_G: 5.9640 D(x): 0.9960 D(G(z)): 0.0158 / 0.0057\n[54/100][400/1583] Loss_D: 0.6703 Loss_G: 4.3031 D(x): 0.6604 D(G(z)): 0.0495 / 0.0552\n[54/100][800/1583] Loss_D: 0.3299 Loss_G: 4.9880 D(x): 0.8113 D(G(z)): 0.0077 / 0.0142\n[54/100][1200/1583] Loss_D: 1.2790 Loss_G: 5.6391 D(x): 0.3963 D(G(z)): 0.0010 / 0.0074\n[55/100][0/1583] Loss_D: 0.1567 Loss_G: 5.4031 D(x): 0.9567 D(G(z)): 0.0292 / 0.0109\n[55/100][400/1583] Loss_D: 0.2051 Loss_G: 3.3811 D(x): 0.9912 D(G(z)): 0.1004 / 0.0587\n[55/100][800/1583] Loss_D: 0.1308 Loss_G: 4.2091 D(x): 0.9887 D(G(z)): 0.0385 / 0.0256\n[55/100][1200/1583] Loss_D: 0.1606 Loss_G: 5.0477 D(x): 0.9933 D(G(z)): 0.0714 / 0.0121\n[56/100][0/1583] Loss_D: 0.1927 Loss_G: 4.6291 D(x): 0.9101 D(G(z)): 0.0132 / 0.0202\n[56/100][400/1583] Loss_D: 0.1099 Loss_G: 5.4889 D(x): 0.9888 D(G(z)): 0.0168 / 0.0097\n[56/100][800/1583] Loss_D: 0.1051 Loss_G: 6.3970 D(x): 0.9968 D(G(z)): 0.0185 / 0.0036\n[56/100][1200/1583] Loss_D: 0.1584 Loss_G: 5.3681 D(x): 0.9422 D(G(z)): 0.0143 / 0.0110\n[57/100][0/1583] Loss_D: 0.2082 Loss_G: 4.0797 D(x): 0.9536 D(G(z)): 0.0674 / 0.0326\n[57/100][400/1583] Loss_D: 0.1568 Loss_G: 4.2536 D(x): 0.9883 D(G(z)): 0.0592 / 0.0303\n[57/100][800/1583] Loss_D: 0.1794 Loss_G: 4.0167 D(x): 0.9629 D(G(z)): 0.0529 / 0.0329\n[57/100][1200/1583] Loss_D: 0.1190 Loss_G: 7.4079 D(x): 0.9626 D(G(z)): 0.0037 / 0.0015\n[58/100][0/1583] Loss_D: 0.1361 Loss_G: 6.4956 D(x): 0.9486 D(G(z)): 0.0014 / 0.0035\n[58/100][400/1583] Loss_D: 0.1517 Loss_G: 6.4677 D(x): 0.9354 D(G(z)): 0.0044 / 0.0041\n[58/100][800/1583] Loss_D: 0.2258 Loss_G: 6.4513 D(x): 0.8734 D(G(z)): 0.0007 / 0.0032\n[58/100][1200/1583] Loss_D: 0.1189 Loss_G: 4.9623 D(x): 0.9884 D(G(z)): 0.0239 / 0.0186\n[59/100][0/1583] Loss_D: 0.1280 Loss_G: 6.0548 D(x): 0.9610 D(G(z)): 0.0077 / 0.0051\n[59/100][400/1583] Loss_D: 0.7627 Loss_G: 8.3640 D(x): 0.9996 D(G(z)): 0.4257 / 0.0005\n[59/100][800/1583] Loss_D: 0.1361 Loss_G: 4.6799 D(x): 0.9729 D(G(z)): 0.0235 / 0.0210\n[59/100][1200/1583] Loss_D: 0.1776 Loss_G: 4.3519 D(x): 0.9939 D(G(z)): 0.0787 / 0.0254\n[60/100][0/1583] Loss_D: 0.1178 Loss_G: 5.3013 D(x): 0.9906 D(G(z)): 0.0326 / 0.0112\n[60/100][400/1583] Loss_D: 0.2065 Loss_G: 5.1559 D(x): 0.9988 D(G(z)): 0.1103 / 0.0104\n[60/100][800/1583] Loss_D: 0.3755 Loss_G: 6.9037 D(x): 0.7786 D(G(z)): 0.0013 / 0.0027\n[60/100][1200/1583] Loss_D: 0.2569 Loss_G: 4.1328 D(x): 0.9824 D(G(z)): 0.1321 / 0.0317\n[61/100][0/1583] Loss_D: 0.0867 Loss_G: 6.0394 D(x): 0.9935 D(G(z)): 0.0114 / 0.0043\n[61/100][400/1583] Loss_D: 0.1046 Loss_G: 8.5780 D(x): 0.9754 D(G(z)): 0.0015 / 0.0004\n[61/100][800/1583] Loss_D: 0.1755 Loss_G: 4.9639 D(x): 0.9278 D(G(z)): 0.0146 / 0.0137\n[61/100][1200/1583] Loss_D: 0.0836 Loss_G: 6.5738 D(x): 0.9966 D(G(z)): 0.0069 / 0.0029\n[62/100][0/1583] Loss_D: 0.1103 Loss_G: 5.3135 D(x): 0.9793 D(G(z)): 0.0111 / 0.0084\n[62/100][400/1583] Loss_D: 0.1412 Loss_G: 7.3503 D(x): 0.9425 D(G(z)): 0.0011 / 0.0014\n[62/100][800/1583] Loss_D: 0.2189 Loss_G: 6.7672 D(x): 0.8851 D(G(z)): 0.0018 / 0.0028\n[62/100][1200/1583] Loss_D: 0.0865 Loss_G: 5.9339 D(x): 0.9985 D(G(z)): 0.0112 / 0.0056\n[63/100][0/1583] Loss_D: 0.0975 Loss_G: 6.3994 D(x): 0.9841 D(G(z)): 0.0058 / 0.0037\n[63/100][400/1583] Loss_D: 0.1036 Loss_G: 5.6661 D(x): 0.9935 D(G(z)): 0.0172 / 0.0081\n[63/100][800/1583] Loss_D: 0.4044 Loss_G: 5.9180 D(x): 0.7736 D(G(z)): 0.0122 / 0.0067\n[63/100][1200/1583] Loss_D: 0.1788 Loss_G: 4.1748 D(x): 0.9918 D(G(z)): 0.0836 / 0.0290\n[64/100][0/1583] Loss_D: 0.6525 Loss_G: 5.5021 D(x): 0.9945 D(G(z)): 0.3686 / 0.0076\n[64/100][400/1583] Loss_D: 0.0900 Loss_G: 6.8016 D(x): 0.9863 D(G(z)): 0.0054 / 0.0026\n[64/100][800/1583] Loss_D: 0.0960 Loss_G: 7.9872 D(x): 0.9796 D(G(z)): 0.0012 / 0.0008\n[64/100][1200/1583] Loss_D: 0.1041 Loss_G: 5.4751 D(x): 0.9810 D(G(z)): 0.0082 / 0.0101\n[65/100][0/1583] Loss_D: 0.1264 Loss_G: 5.7694 D(x): 0.9662 D(G(z)): 0.0107 / 0.0078\n[65/100][400/1583] Loss_D: 1.1808 Loss_G: 3.1931 D(x): 0.8190 D(G(z)): 0.4160 / 0.0733\n[65/100][800/1583] Loss_D: 0.1013 Loss_G: 5.9160 D(x): 0.9857 D(G(z)): 0.0123 / 0.0055\n[65/100][1200/1583] Loss_D: 0.2027 Loss_G: 2.6846 D(x): 0.9879 D(G(z)): 0.0937 / 0.1267\n[66/100][0/1583] Loss_D: 0.2578 Loss_G: 3.2191 D(x): 0.9988 D(G(z)): 0.1487 / 0.0736\n[66/100][400/1583] Loss_D: 1.3436 Loss_G: 3.9504 D(x): 0.3689 D(G(z)): 0.0027 / 0.0493\n[66/100][800/1583] Loss_D: 0.1564 Loss_G: 6.3062 D(x): 0.9316 D(G(z)): 0.0024 / 0.0042\n[66/100][1200/1583] Loss_D: 0.0812 Loss_G: 6.9221 D(x): 0.9964 D(G(z)): 0.0046 / 0.0023\n[67/100][0/1583] Loss_D: 0.1019 Loss_G: 5.2927 D(x): 0.9869 D(G(z)): 0.0127 / 0.0100\n[67/100][400/1583] Loss_D: 2.8745 Loss_G: 3.0600 D(x): 0.2493 D(G(z)): 0.0419 / 0.1498\n[67/100][800/1583] Loss_D: 0.1996 Loss_G: 5.2134 D(x): 0.9394 D(G(z)): 0.0436 / 0.0100\n[67/100][1200/1583] Loss_D: 0.7563 Loss_G: 5.8727 D(x): 0.5771 D(G(z)): 0.0009 / 0.0068\n[68/100][0/1583] Loss_D: 0.0987 Loss_G: 4.7015 D(x): 0.9986 D(G(z)): 0.0244 / 0.0168\n[68/100][400/1583] Loss_D: 0.1933 Loss_G: 4.9380 D(x): 0.9094 D(G(z)): 0.0094 / 0.0168\n[68/100][800/1583] Loss_D: 0.0988 Loss_G: 7.2022 D(x): 0.9821 D(G(z)): 0.0029 / 0.0016\n[68/100][1200/1583] Loss_D: 0.1175 Loss_G: 5.6838 D(x): 0.9660 D(G(z)): 0.0068 / 0.0072\n[69/100][0/1583] Loss_D: 0.1300 Loss_G: 4.8811 D(x): 0.9783 D(G(z)): 0.0257 / 0.0158\n[69/100][400/1583] Loss_D: 0.0922 Loss_G: 6.2516 D(x): 0.9902 D(G(z)): 0.0073 / 0.0052\n[69/100][800/1583] Loss_D: 0.0945 Loss_G: 5.1781 D(x): 0.9972 D(G(z)): 0.0153 / 0.0139\n[69/100][1200/1583] Loss_D: 0.0944 Loss_G: 6.3148 D(x): 0.9927 D(G(z)): 0.0123 / 0.0042\n[70/100][0/1583] Loss_D: 0.1659 Loss_G: 6.3513 D(x): 0.9223 D(G(z)): 0.0024 / 0.0040\n[70/100][400/1583] Loss_D: 0.1270 Loss_G: 5.5137 D(x): 0.9926 D(G(z)): 0.0376 / 0.0097\n[70/100][800/1583] Loss_D: 1.0737 Loss_G: 3.7462 D(x): 0.9815 D(G(z)): 0.5350 / 0.0436\n[70/100][1200/1583] Loss_D: 0.1391 Loss_G: 4.7092 D(x): 0.9696 D(G(z)): 0.0218 / 0.0250\n[71/100][0/1583] Loss_D: 0.1063 Loss_G: 5.7371 D(x): 0.9806 D(G(z)): 0.0144 / 0.0067\n[71/100][400/1583] Loss_D: 0.1220 Loss_G: 6.9123 D(x): 0.9579 D(G(z)): 0.0020 / 0.0019\n[71/100][800/1583] Loss_D: 0.1349 Loss_G: 5.5681 D(x): 0.9539 D(G(z)): 0.0095 / 0.0097\n[71/100][1200/1583] Loss_D: 0.1892 Loss_G: 3.7794 D(x): 0.9993 D(G(z)): 0.0989 / 0.0429\n[72/100][0/1583] Loss_D: 0.1054 Loss_G: 5.5126 D(x): 0.9921 D(G(z)): 0.0244 / 0.0091\n[72/100][400/1583] Loss_D: 0.3274 Loss_G: 3.7529 D(x): 0.8197 D(G(z)): 0.0308 / 0.0489\n[72/100][800/1583] Loss_D: 0.2977 Loss_G: 3.3038 D(x): 0.8743 D(G(z)): 0.0628 / 0.0618\n[72/100][1200/1583] Loss_D: 0.1188 Loss_G: 5.2252 D(x): 0.9721 D(G(z)): 0.0149 / 0.0093\n[73/100][0/1583] Loss_D: 0.0897 Loss_G: 9.5378 D(x): 0.9813 D(G(z)): 0.0003 / 0.0002\n[73/100][400/1583] Loss_D: 0.0820 Loss_G: 8.6406 D(x): 0.9886 D(G(z)): 0.0007 / 0.0005\n[73/100][800/1583] Loss_D: 0.2466 Loss_G: 5.8768 D(x): 0.9990 D(G(z)): 0.1463 / 0.0045\n[73/100][1200/1583] Loss_D: 0.2404 Loss_G: 3.4220 D(x): 0.8702 D(G(z)): 0.0200 / 0.0663\n[74/100][0/1583] Loss_D: 0.3027 Loss_G: 4.1242 D(x): 0.9910 D(G(z)): 0.1735 / 0.0311\n[74/100][400/1583] Loss_D: 0.0954 Loss_G: 5.6037 D(x): 0.9832 D(G(z)): 0.0083 / 0.0085\n[74/100][800/1583] Loss_D: 0.0859 Loss_G: 8.3468 D(x): 0.9881 D(G(z)): 0.0021 / 0.0007\n[74/100][1200/1583] Loss_D: 0.3776 Loss_G: 3.1841 D(x): 0.9823 D(G(z)): 0.2192 / 0.0767\n[75/100][0/1583] Loss_D: 1.0196 Loss_G: 3.0640 D(x): 0.9228 D(G(z)): 0.5117 / 0.0712\n[75/100][400/1583] Loss_D: 0.0939 Loss_G: 5.1801 D(x): 0.9943 D(G(z)): 0.0181 / 0.0110\n[75/100][800/1583] Loss_D: 0.1034 Loss_G: 5.9300 D(x): 0.9751 D(G(z)): 0.0070 / 0.0064\n[75/100][1200/1583] Loss_D: 2.5742 Loss_G: 9.3336 D(x): 0.9992 D(G(z)): 0.8643 / 0.0003\n[76/100][0/1583] Loss_D: 0.1061 Loss_G: 6.8062 D(x): 0.9742 D(G(z)): 0.0035 / 0.0027\n[76/100][400/1583] Loss_D: 0.1025 Loss_G: 8.0487 D(x): 0.9677 D(G(z)): 0.0012 / 0.0007\n[76/100][800/1583] Loss_D: 0.1901 Loss_G: 5.6151 D(x): 0.9023 D(G(z)): 0.0017 / 0.0092\n[76/100][1200/1583] Loss_D: 0.1046 Loss_G: 5.7740 D(x): 0.9768 D(G(z)): 0.0052 / 0.0081\n[77/100][0/1583] Loss_D: 0.4043 Loss_G: 3.3238 D(x): 0.7744 D(G(z)): 0.0201 / 0.0653\n[77/100][400/1583] Loss_D: 0.1367 Loss_G: 5.0266 D(x): 0.9819 D(G(z)): 0.0394 / 0.0141\n[77/100][800/1583] Loss_D: 0.1458 Loss_G: 4.8409 D(x): 0.9945 D(G(z)): 0.0615 / 0.0144\n[77/100][1200/1583] Loss_D: 0.0844 Loss_G: 5.7290 D(x): 0.9985 D(G(z)): 0.0132 / 0.0071\n[78/100][0/1583] Loss_D: 0.1252 Loss_G: 5.6635 D(x): 0.9997 D(G(z)): 0.0504 / 0.0069\n[78/100][400/1583] Loss_D: 0.0910 Loss_G: 6.4547 D(x): 0.9927 D(G(z)): 0.0081 / 0.0032\n[78/100][800/1583] Loss_D: 0.2932 Loss_G: 3.5186 D(x): 0.8765 D(G(z)): 0.0617 / 0.0605\n[78/100][1200/1583] Loss_D: 0.0993 Loss_G: 4.9489 D(x): 0.9980 D(G(z)): 0.0239 / 0.0163\n[79/100][0/1583] Loss_D: 0.0815 Loss_G: 5.6658 D(x): 0.9993 D(G(z)): 0.0154 / 0.0064\n[79/100][400/1583] Loss_D: 0.0892 Loss_G: 8.5604 D(x): 0.9834 D(G(z)): 0.0007 / 0.0005\n[79/100][800/1583] Loss_D: 0.0885 Loss_G: 6.7623 D(x): 0.9935 D(G(z)): 0.0107 / 0.0022\n[79/100][1200/1583] Loss_D: 0.0871 Loss_G: 6.3091 D(x): 0.9939 D(G(z)): 0.0097 / 0.0049\n[80/100][0/1583] Loss_D: 0.0670 Loss_G: 7.7320 D(x): 0.9994 D(G(z)): 0.0016 / 0.0009\n[80/100][400/1583] Loss_D: 0.2369 Loss_G: 7.0517 D(x): 0.8607 D(G(z)): 0.0005 / 0.0022\n[80/100][800/1583] Loss_D: 0.0917 Loss_G: 7.1910 D(x): 0.9857 D(G(z)): 0.0080 / 0.0017\n[80/100][1200/1583] Loss_D: 0.1598 Loss_G: 6.9039 D(x): 0.9365 D(G(z)): 0.0164 / 0.0029\n[81/100][0/1583] Loss_D: 0.1405 Loss_G: 7.1855 D(x): 0.9312 D(G(z)): 0.0008 / 0.0016\n[81/100][400/1583] Loss_D: 0.0985 Loss_G: 5.6566 D(x): 0.9890 D(G(z)): 0.0189 / 0.0079\n[81/100][800/1583] Loss_D: 0.2294 Loss_G: 3.4534 D(x): 0.9284 D(G(z)): 0.0752 / 0.0560\n[81/100][1200/1583] Loss_D: 0.1378 Loss_G: 4.8809 D(x): 0.9561 D(G(z)): 0.0179 / 0.0168\n[82/100][0/1583] Loss_D: 0.0903 Loss_G: 7.7157 D(x): 0.9855 D(G(z)): 0.0022 / 0.0011\n[82/100][400/1583] Loss_D: 0.1106 Loss_G: 6.8214 D(x): 0.9624 D(G(z)): 0.0025 / 0.0020\n[82/100][800/1583] Loss_D: 0.3033 Loss_G: 4.3220 D(x): 0.8351 D(G(z)): 0.0238 / 0.0243\n[82/100][1200/1583] Loss_D: 0.0909 Loss_G: 7.7416 D(x): 0.9779 D(G(z)): 0.0014 / 0.0011\n[83/100][0/1583] Loss_D: 0.5177 Loss_G: 3.4671 D(x): 0.6857 D(G(z)): 0.0001 / 0.0610\n[83/100][400/1583] Loss_D: 0.4859 Loss_G: 3.0881 D(x): 0.7621 D(G(z)): 0.0352 / 0.0888\n[83/100][800/1583] Loss_D: 0.1514 Loss_G: 5.6418 D(x): 0.9449 D(G(z)): 0.0171 / 0.0090\n[83/100][1200/1583] Loss_D: 0.1351 Loss_G: 4.4163 D(x): 0.9628 D(G(z)): 0.0211 / 0.0241\n[84/100][0/1583] Loss_D: 0.0688 Loss_G: 7.4922 D(x): 0.9975 D(G(z)): 0.0026 / 0.0012\n[84/100][400/1583] Loss_D: 0.1183 Loss_G: 3.9695 D(x): 0.9970 D(G(z)): 0.0424 / 0.0375\n[84/100][800/1583] Loss_D: 0.1295 Loss_G: 6.7955 D(x): 0.9489 D(G(z)): 0.0031 / 0.0026\n[84/100][1200/1583] Loss_D: 0.1101 Loss_G: 5.0180 D(x): 0.9823 D(G(z)): 0.0181 / 0.0130\n[85/100][0/1583] Loss_D: 0.4476 Loss_G: 5.0251 D(x): 0.7709 D(G(z)): 0.0065 / 0.0148\n[85/100][400/1583] Loss_D: 0.1310 Loss_G: 6.5877 D(x): 0.9496 D(G(z)): 0.0059 / 0.0039\n[85/100][800/1583] Loss_D: 0.1375 Loss_G: 4.8173 D(x): 0.9998 D(G(z)): 0.0641 / 0.0159\n[85/100][1200/1583] Loss_D: 0.1044 Loss_G: 5.3215 D(x): 0.9999 D(G(z)): 0.0344 / 0.0102\n[86/100][0/1583] Loss_D: 0.2231 Loss_G: 3.4788 D(x): 0.8770 D(G(z)): 0.0052 / 0.0645\n[86/100][400/1583] Loss_D: 0.0746 Loss_G: 6.2539 D(x): 0.9994 D(G(z)): 0.0085 / 0.0049\n[86/100][800/1583] Loss_D: 0.1237 Loss_G: 5.1127 D(x): 0.9979 D(G(z)): 0.0491 / 0.0124\n[86/100][1200/1583] Loss_D: 0.1069 Loss_G: 5.8189 D(x): 0.9765 D(G(z)): 0.0118 / 0.0066\n[87/100][0/1583] Loss_D: 0.0782 Loss_G: 8.5160 D(x): 0.9907 D(G(z)): 0.0011 / 0.0006\n[87/100][400/1583] Loss_D: 0.0921 Loss_G: 10.4933 D(x): 0.9762 D(G(z)): 0.0003 / 0.0001\n[87/100][800/1583] Loss_D: 0.0681 Loss_G: 8.3139 D(x): 0.9990 D(G(z)): 0.0015 / 0.0005\n[87/100][1200/1583] Loss_D: 0.1107 Loss_G: 5.5650 D(x): 0.9910 D(G(z)): 0.0324 / 0.0079\n[88/100][0/1583] Loss_D: 0.3837 Loss_G: 4.5610 D(x): 0.7660 D(G(z)): 0.0121 / 0.0211\n[88/100][400/1583] Loss_D: 1.3433 Loss_G: 1.2075 D(x): 0.3697 D(G(z)): 0.0014 / 0.4174\n[88/100][800/1583] Loss_D: 0.3157 Loss_G: 3.4734 D(x): 0.8195 D(G(z)): 0.0058 / 0.0641\n[88/100][1200/1583] Loss_D: 0.1069 Loss_G: 5.9295 D(x): 0.9908 D(G(z)): 0.0271 / 0.0062\n[89/100][0/1583] Loss_D: 0.1041 Loss_G: 6.4779 D(x): 0.9724 D(G(z)): 0.0064 / 0.0035\n[89/100][400/1583] Loss_D: 0.1005 Loss_G: 7.1448 D(x): 0.9740 D(G(z)): 0.0028 / 0.0020\n[89/100][800/1583] Loss_D: 0.0701 Loss_G: 8.1578 D(x): 0.9992 D(G(z)): 0.0012 / 0.0007\n[89/100][1200/1583] Loss_D: 0.0921 Loss_G: 8.6219 D(x): 0.9793 D(G(z)): 0.0008 / 0.0005\n[90/100][0/1583] Loss_D: 0.1180 Loss_G: 7.6996 D(x): 0.9497 D(G(z)): 0.0010 / 0.0009\n[90/100][400/1583] Loss_D: 0.0762 Loss_G: 6.6813 D(x): 0.9975 D(G(z)): 0.0060 / 0.0044\n[90/100][800/1583] Loss_D: 0.0978 Loss_G: 9.4607 D(x): 0.9706 D(G(z)): 0.0002 / 0.0002\n[90/100][1200/1583] Loss_D: 0.0837 Loss_G: 6.6769 D(x): 0.9853 D(G(z)): 0.0037 / 0.0026\n[91/100][0/1583] Loss_D: 0.1237 Loss_G: 6.0613 D(x): 0.9528 D(G(z)): 0.0054 / 0.0057\n[91/100][400/1583] Loss_D: 0.3034 Loss_G: 10.5587 D(x): 0.9999 D(G(z)): 0.2000 / 0.0000\n[91/100][800/1583] Loss_D: 0.1302 Loss_G: 8.0544 D(x): 0.9433 D(G(z)): 0.0010 / 0.0009\n[91/100][1200/1583] Loss_D: 0.1120 Loss_G: 5.8629 D(x): 0.9737 D(G(z)): 0.0142 / 0.0068\n[92/100][0/1583] Loss_D: 0.0808 Loss_G: 5.6120 D(x): 0.9966 D(G(z)): 0.0096 / 0.0082\n[92/100][400/1583] Loss_D: 0.1266 Loss_G: 6.6965 D(x): 0.9477 D(G(z)): 0.0029 / 0.0024\n[92/100][800/1583] Loss_D: 0.1310 Loss_G: 4.3748 D(x): 0.9770 D(G(z)): 0.0336 / 0.0224\n[92/100][1200/1583] Loss_D: 0.2766 Loss_G: 7.3288 D(x): 0.8460 D(G(z)): 0.0005 / 0.0019\n[93/100][0/1583] Loss_D: 0.0817 Loss_G: 5.9117 D(x): 0.9966 D(G(z)): 0.0102 / 0.0055\n[93/100][400/1583] Loss_D: 0.1564 Loss_G: 5.0936 D(x): 0.9815 D(G(z)): 0.0567 / 0.0155\n[93/100][800/1583] Loss_D: 0.0821 Loss_G: 5.9188 D(x): 0.9977 D(G(z)): 0.0120 / 0.0078\n[93/100][1200/1583] Loss_D: 0.1085 Loss_G: 6.0226 D(x): 0.9706 D(G(z)): 0.0090 / 0.0054\n[94/100][0/1583] Loss_D: 0.0696 Loss_G: 8.4083 D(x): 0.9940 D(G(z)): 0.0007 / 0.0005\n[94/100][400/1583] Loss_D: 0.0892 Loss_G: 6.7405 D(x): 0.9892 D(G(z)): 0.0081 / 0.0026\n[94/100][800/1583] Loss_D: 0.1106 Loss_G: 4.9044 D(x): 0.9968 D(G(z)): 0.0390 / 0.0135\n[94/100][1200/1583] Loss_D: 0.0760 Loss_G: 9.3057 D(x): 0.9885 D(G(z)): 0.0005 / 0.0002\n[95/100][0/1583] Loss_D: 0.4202 Loss_G: 5.7608 D(x): 0.7657 D(G(z)): 0.0077 / 0.0068\n[95/100][400/1583] Loss_D: 0.1149 Loss_G: 5.0762 D(x): 0.9998 D(G(z)): 0.0479 / 0.0126\n[95/100][800/1583] Loss_D: 0.3299 Loss_G: 3.5339 D(x): 0.8914 D(G(z)): 0.1014 / 0.0609\n[95/100][1200/1583] Loss_D: 0.0915 Loss_G: 7.0966 D(x): 0.9814 D(G(z)): 0.0038 / 0.0019\n[96/100][0/1583] Loss_D: 0.0680 Loss_G: 7.1194 D(x): 0.9983 D(G(z)): 0.0048 / 0.0016\n[96/100][400/1583] Loss_D: 0.1594 Loss_G: 4.3844 D(x): 0.9425 D(G(z)): 0.0248 / 0.0241\n[96/100][800/1583] Loss_D: 0.0840 Loss_G: 5.3669 D(x): 0.9970 D(G(z)): 0.0175 / 0.0099\n[96/100][1200/1583] Loss_D: 0.3005 Loss_G: 8.8345 D(x): 0.8299 D(G(z)): 0.0029 / 0.0004\n[97/100][0/1583] Loss_D: 0.1244 Loss_G: 5.4640 D(x): 0.9564 D(G(z)): 0.0124 / 0.0099\n[97/100][400/1583] Loss_D: 0.0739 Loss_G: 6.2577 D(x): 0.9998 D(G(z)): 0.0078 / 0.0056\n[97/100][800/1583] Loss_D: 0.1301 Loss_G: 6.4619 D(x): 0.9553 D(G(z)): 0.0118 / 0.0070\n[97/100][1200/1583] Loss_D: 0.0747 Loss_G: 6.3038 D(x): 0.9949 D(G(z)): 0.0062 / 0.0045\n[98/100][0/1583] Loss_D: 0.0771 Loss_G: 6.8954 D(x): 0.9890 D(G(z)): 0.0030 / 0.0022\n[98/100][400/1583] Loss_D: 0.1018 Loss_G: 6.2145 D(x): 0.9745 D(G(z)): 0.0069 / 0.0046\n[98/100][800/1583] Loss_D: 0.0915 Loss_G: 6.7000 D(x): 0.9821 D(G(z)): 0.0046 / 0.0033\n[98/100][1200/1583] Loss_D: 0.0833 Loss_G: 9.4610 D(x): 0.9848 D(G(z)): 0.0003 / 0.0002\n[99/100][0/1583] Loss_D: 0.0799 Loss_G: 6.5083 D(x): 0.9893 D(G(z)): 0.0071 / 0.0031\n[99/100][400/1583] Loss_D: 0.0926 Loss_G: 5.8461 D(x): 0.9823 D(G(z)): 0.0062 / 0.0061\n[99/100][800/1583] Loss_D: 0.0654 Loss_G: 8.4868 D(x): 0.9960 D(G(z)): 0.0018 / 0.0004\n[99/100][1200/1583] Loss_D: 0.0674 Loss_G: 6.7760 D(x): 0.9939 D(G(z)): 0.0034 / 0.0020\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Fe7riNvWoko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_images(tensor):\n",
        "\n",
        "    min_val = float(tensor.min())\n",
        "    max_val = float(tensor.max())\n",
        "    tensor.clamp_(min=min_val, max=max_val)\n",
        "    tensor.add_(-min_val).div_(max_val-min_val+1e-5)\n",
        "\n",
        "    images = (\n",
        "        tensor\n",
        "        .mul_(255)\n",
        "        .add_(0.5)\n",
        "        .clamp_(0, 255)\n",
        "        .permute(0, 2, 3, 1)\n",
        "        .to(\"cpu\", dtype=torch.uint8)\n",
        "        .numpy()\n",
        "    )\n",
        "\n",
        "    return images"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BibIj2CnhmUR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_quantizer(model, optimzier, niters, fname):\n",
        "    model.train()\n",
        "    with tqdm(range(niters), position=0, leave=True) as pbar:\n",
        "        for idx in pbar:\n",
        "\n",
        "            quantize, loss, embedding_idx = model(torch.load(fname).detach())\n",
        "\n",
        "            model.zero_grad()\n",
        "            if loss.grad_fn is not None: \n",
        "                loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            pbar.set_description(f\"loss: {loss.item():.6f}\")\n",
        "\n",
        "    return quantize.cpu(), loss.cpu(), embedding_idx.cpu()"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZxLRGN3l6DN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_histgram(embedding_idx):\n",
        "    indexes, values = np.unique(embedding_idx.numpy(), return_counts=True)\n",
        "    return indexes, values"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHV07M0brqbX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "VectorQuantizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM61PZLMXxANR8oUXecDiEE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6727ecf8889443a7bbff63a62667925e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0da1406e59af48058f047100c7d1a886",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e292a8af7ec44cd8ae08227170a7cd4f",
              "IPY_MODEL_fe61109823124397b661ce95f3475f20"
            ]
          }
        },
        "0da1406e59af48058f047100c7d1a886": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e292a8af7ec44cd8ae08227170a7cd4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e3c0c1467146463ead540a6e72ee3eee",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_27e54298f74f412893fe519cbba84f25"
          }
        },
        "fe61109823124397b661ce95f3475f20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d887e3f6974c4195b85e129270d23087",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:30&lt;00:00, 16454730.66it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ef339de6ad474e7f8cce5ba593608617"
          }
        },
        "e3c0c1467146463ead540a6e72ee3eee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "27e54298f74f412893fe519cbba84f25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d887e3f6974c4195b85e129270d23087": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ef339de6ad474e7f8cce5ba593608617": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}